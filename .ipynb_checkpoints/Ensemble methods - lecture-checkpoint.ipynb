{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees: ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembles combine multiple machine learning models to create more powerful models.\n",
    "\n",
    "Althoughs there is an endless amount of models in the machine learning literature that belong to this category, there are three ensemble models that have proven to be effective on a wide range of datasets for classification and regression, both of which use decision trees as their building blocks: bagged trees, random forests and gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bagged trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagged trees are basically the easiest way to improve your results and make your model more robust. By using bagged trees, you generate a decision tree on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Forests: the concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data overfitting is common in decision trees. Random Forests use the idea that while each tree might do pretty well making predictions, it will probably overfit on part of the data. Random forests use a collection of trees. The idea is that, by randomizing over different trees that all differ slightly, averaging their result will lead to a better solution.\n",
    "\n",
    "To get to trees that differ slightly, randomness should be injected into the tree-building mechanism. Random forests use two principles to generate trees that are different:\n",
    "\n",
    "\n",
    "- For each tree, we can use a different subset of data. (which is basically what bagging does)\n",
    "- For each tree, we can only include a few features present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building a random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps to build a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decide how many trees you'll use to create the random forest.\n",
    "2. Make a \"bootstrap sample\" for each tree: if you have $n$ data points in the original data, you randomly draw $n$ samples from the data, \"with replacement\". This means that cases can be drawn multiple times, and some will not be drawn at all.\n",
    "3. Next, you build a decision tree based on the newly created dataset. When constructing random forests, however, the standard algorithm for decision tree construction is slightly altered. Instead for looking across *all* features to create a split, it randomly selects a subset of all the available features, and looks for the best split within this subset. Next, the number of features that is selected can be set by the `max_features` parameter in scikit-learn. In each new node, a new subset of features is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "- `max_features` should be smaller than the total amount of features present in the data set. (using a high (trees will be similar) vs low `max_features`(trees will be different)) .\n",
    "- To make a prediction using the random forest, first, a prediction is made for every tree in the forest. \n",
    "    - Regression: results are simply averaged to get to the final prediction. \n",
    "    - Classification, a “soft voting” strategy is used: Each algorithm makes a “soft” prediction, providing probabilities for each class. Once averaged over all the probabilities, the class with the highest probability is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting methods also combine multiple decision trees to create more powerful models. Unlike random forests, gradient boosting builds trees in a serial way, trying to correct for mistakes in the previous tree. As a result, there is no such thing as randomized trees, and generally a solid amount of pre-tuning is used. When performing  boosting, often very shallow trees are generated, with a maximum depth of 5. This way, predictions are fairly easy, which speeds up the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Gradient Tree Boosting or Gradient Boosted Regression Trees (GBRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essential building block in GBTRs are so-called \"weak learners\", in this case fairly shallow trees.\n",
    "When using GBRT, a loss function is being minimized, and the objective is to minimize the loss of the model by adding weak learners using a procedure based on gradient descent.\n",
    "\n",
    "As with regular trees, the trees are constructed in a greedy manner. Because trees are generated sequentially, it is important that the trees aren't too big and/or the trees are constrained in certain ways, such as constraining the number of cases for splits or leaf nodes. Each iterations predictions are made on part of the data, so by adding more and more trees the performance can be improved iteratively.\n",
    "\n",
    "Like Random forests, the final model is basically a collection of trees. For boosting methods, trees are added one at a time, and existing trees (even though we know initial trees might be truly weak performers) are not changed. \n",
    "\n",
    "A loss function is used. and we have weak learner submmodels, more specifically, decision trees. Every time a tree is added, the loss is calculated. Using gradient descent, a new tree is added in order to improve the model (which is, reduces the loss). This is essentially done by parametrizing the tree, and moving it to the right direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosted trees are very popular in machine learning competitions and widely used. A downside is that (unlike with random forests), hyperparameters need to be chosen wisely, as the accuracy results strongly depend on that. One important hyperparameter is the learning rate associated with the gradient boosting. A higher learning rate will lead to more intense mistake corrections compared to previous trees for each step in the algorithm. This will lead to models that are more complex. A bigger `n_estimators` \n",
    "\n",
    "When using gradient boosting, by default 100 trees with maximum depth of 3 and a learning rate of 0.1 are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special case of Gradient Boosting Trees is the Adaboost algorithm. AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. This way, in each iteration, the new weak learner will focus increasingly on instances that are hard to classify. \n",
    "\n",
    "Initially, each instance carries an equal weight, and these weights are gradually changed as the algorithm progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.svds.com/machine-learning-vs-statistics/\n",
    "https://github.com/xbno/Projects/blob/master/Models_Scratch/Decision%20Trees%20from%20scratch.ipynb\n",
    "http://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
