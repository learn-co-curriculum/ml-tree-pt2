{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees: ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembles combine multiple machine learning models to create more powerful models.\n",
    "\n",
    "Althoughs there is an endless amount of models in the machine learning literature that belong to this category, there are two ensemble models that have proven to be effective on a wide range of datasets for classification and regression, both of which use decision trees as their building blocks: random forests and gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bagged trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Forests: the concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data overfitting is common in decision trees. Random Forests use the idea that while each tree might do pretty well making predictions, it will probably overfit on part of the data. Random forests use a collection of trees. The idea is that, by randomizing over different trees that all differ slightly, averaging their result will lead to a better solution.\n",
    "\n",
    "To get to trees that differ slightly, randomness should be injected into the treebuilding mechanism. There are two ways to do this:\n",
    "- For each tree, we can use a different subset of data.\n",
    "- For each tree, we can only include a few features present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building a random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps to build a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decide how many trees you'll use to create the random forest.\n",
    "2. Make a \"bootstrap sample\" for each tree: if you have $n$ data points in the original data, you randomly draw $n$ samples from the data, \"with replacement\". This means that cases can be drawn multiple times, and some will not be drawn at all.\n",
    "3. Next, you build a decision tree based on the newly created dataset. When constructing random forests, however, the standard algorithm for decision tree construction is slightly altered. Instead for looking across *all* features to create a split, it randomly selects a subset of all the available features, and looks for the best split within this subset. Next, the number of features that is selected can be set by the `max_features` parameter in scikit-learn. In each new node, a new subset of features is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "- `max_features` should be smaller than the total amount of features present in the data set. (using a high (trees will be similar) vs low `max_features`(trees will be different)) .\n",
    "- To make a prediction using the random forest, first, a prediction is made for every tree in the forest. \n",
    "    - Regression: results are simply averaged to get to the final prediction. \n",
    "    - Classification, a “soft voting” strategy is used: Each algorithm makes a “soft” prediction, providing probabilities for each class. Once averaged over all the probabilities, the class with the highest probability is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.svds.com/machine-learning-vs-statistics/\n",
    "https://github.com/xbno/Projects/blob/master/Models_Scratch/Decision%20Trees%20from%20scratch.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
